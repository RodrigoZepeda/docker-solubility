#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Apr 29 18:23:37 2019

@author: rod

Based on
https://deepchem.io/docs/2.0.0/_modules/deepchem/molnet/load_function/delaney_datasets.html
https://github.com/deepchem/deepchem/blob/master/examples/delaney/delaney_graph_conv.py

Check, for inspiration
https://pchanda.github.io/Deepchem-GraphConvolutions/
https://www.deepchem.io/docs/notebooks/graph_convolutional_networks_for_tox21.html
https://www.oreilly.com/library/view/deep-learning-for/9781492039822/ch04.html

For interpretability
https://deepchem.io/docs/notebooks/Explaining_Tox21.html

Random forest:
https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d
"""
from __future__ import print_function
from __future__ import division
from __future__ import unicode_literals

import numpy as np
np.random.seed(52316)

import sys
import deepchem as dc
from deepchem.models.sklearn_models import RandomForestRegressor, SklearnModel
from deepchem.utils.save import load_from_disk

import pandas as pd
import os
#Count number of processors of server
import multiprocessing
cpus = multiprocessing.cpu_count()

"""
DATASETS
"""
dataset_file= "Complete_dataset_without_duplicates.csv"
modeldir = "random_forest/"
nestimators = 10

#Create directory if not exists
if not os.path.exists(modeldir):
    os.makedirs(modeldir)

#Read dataset
dataset = load_from_disk(dataset_file)

#Establish featurizer
featurizer = dc.feat.fingerprints.CircularFingerprint(size=1024)

#Read CSV with featurizer
loader = dc.data.CSVLoader(
      tasks=["logS"],
      smiles_field="smiles",
      featurizer=featurizer)

#dataset = loader.featurize(dataset_file,  shard_size=8192)
dataset = loader.featurize(dataset_file)

# Initialize transformers
transformers = [
      dc.trans.NormalizationTransformer(
          transform_y=True, dataset=dataset)
]

for transformer in transformers:
   dataset = transformer.transform(dataset)

#Split dataset according to index (why?)
splitter = dc.splits.IndexSplitter(dataset_file)
train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(
    dataset)

"""
MODEL BUILDING
"""



# Fit using median as median is kept by exponential (ln S)
metric = dc.metrics.Metric(dc.metrics.pearson_r2_score, np.median)

# Do setup for optimal parameter searching
n_estimators       = int(pow(2,10))
max_features       = "auto" #Empirical

#Create model
sklmodel = RandomForestRegressor(n_estimators=n_estimators,
                                 criterion = "mse",
                                 max_features = max_features,
                                 bootstrap = True,
                                 oob_score = False,
                                 n_jobs = int(cpus/2))
model = SklearnModel(sklmodel, modeldir)
model.fit(train_dataset)

#Append trains cores and results
train_scores  = model.evaluate(train_dataset, [metric, dc.metrics.Metric(dc.metrics.mae_score)])
valid_scores  = model.evaluate(valid_dataset, [metric, dc.metrics.Metric(dc.metrics.mae_score)])

#Create prediction
predictions_t = model.predict(train_dataset)
predictions = model.predict(test_dataset)

from sklearn.metrics import r2_score
from scipy.stats import pearsonr

print('Train Sklearn R²:', round(r2_score(predictions_t, train_dataset.y) , 2))
print('Train Deepchems R²:', round(pearsonr(predictions_t, train_dataset.y.flatten())[0]**2 , 2)) #This is what deepchem uses

print('Pred Sklearn R²:', round(r2_score(predictions, test_dataset.y) , 2))
print('Pred Deepchems R²:', round(pearsonr(predictions, test_dataset.y.flatten())[0]**2 , 2)) #This is what deepchem uses

"""
#Save to compare sample with skmodel
database_exp_t = pd.DataFrame(train_dataset.y)
database_exp_v = pd.DataFrame(test_dataset.y)

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators = nestimators,
                             criterion = "mse",
                             max_features = "auto",
                             bootstrap = True,
                             oob_score = False,
                             n_jobs = int(cpus/2),
                             verbose = 1)
datTrain = pd.DataFrame(train_dataset.X)
datTest  = pd.DataFrame(test_dataset.X)

trainset = pd.concat([database_exp_t, datTrain], axis = 1)
testset  = pd.concat([database_exp_v, datTest], axis = 1)

#Save to compare
trainset.to_csv("train_deepchem.csv", header = False, index = False)
testset.to_csv("test_deepchem.csv", header = False, index = False)


rf.fit(datTrain, database_exp_t)

predictions2   = rf.predict(datTest)
predictions2_t = rf.predict(datTrain)


print('Train Sklearn R²:', round(r2_score(predictions2_t, database_exp_t) , 2))
print('Train Deepchems R²:', round(pearsonr(predictions2_t, train_dataset.y.flatten())[0]**2 , 2)) #This is what deepchem uses

print('Pred Sklearn R²:', round(r2_score(predictions2, database_exp_v) , 2))
print('Pred Deepchems R²:', round(pearsonr(predictions2, test_dataset.y.flatten())[0]**2 , 2)) #This is what deepchem uses

"""

