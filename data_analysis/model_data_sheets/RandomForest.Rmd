---
title: "Random Forest"
output: github_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library("knitr")
library("stringr")
model_name <- "Random Forest"
dir_name   <- "~/Dropbox/Quimica/Docker/docker-solubility/data_analysis/built_models/"
short_name <- gsub(" ","",model_name)
model_dir  <- gsub(" ", "_", tolower(model_name))
model_path <- paste0(dir_name,model_dir)
  
knitr::opts_knit$set(root.dir = model_path)

library("ggplot2")
library("reshape2")
library("grid")
library("gridExtra")

#extract legend
#https://github.com/hadley/ggplot2/wiki/Share-a-legend-between-two-ggplot2-graphs
g_legend<-function(a.gplot){
  tmp <- ggplot_gtable(ggplot_build(a.gplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)}
```

## Model Description
Here goes the model explanation

## Model metrics

Several models were fitted to asses convergence. R² was used as the model's metric. 

```{r, echo = FALSE}
mymodel <- read.csv(paste0(short_name,"Analysis.csv"))
mymodel <- melt(mymodel, id.vars=c("metric", "nestimators"))
maeplot <- ggplot(subset(mymodel, metric == "mse")) +
            geom_line(aes(x = round(log(nestimators,2)), y = value, color = variable)) + 
            theme_bw() + ylim(c(0,1)) +
            ggtitle("Mean Absolute Error (MAE)\nfor fitted model") + 
            xlab("log2 number of estimators") + 
            ylab("MAE")
r2plot <- ggplot(subset(mymodel, metric == "r2")) +
            geom_line(aes(x = round(log(nestimators,2)), y = value, color = variable)) + 
            theme_bw() + ylim(c(0,1)) +
            ggtitle("R²\nfor fitted model") + 
            xlab("log2 number of estimators") +
            ylab("R²")

mylegend<-g_legend(maeplot)

grid.arrange(arrangeGrob(
                    maeplot + theme(legend.position="none"),
                    r2plot + theme(legend.position="none"), nrow=1),
             mylegend, nrow=2, heights=c(6, 2))
```

The following table compares the errors in both the training and the validation sets for the model with 2^`r round(log(max(mymodel$nestimators),2))` estimators.

```{r, echo = FALSE}
#Get prediction data
vval_pred_file  <- paste0("predict_validation_",round(max(mymodel$nestimators)),".csv")
tval_pred_file  <- paste0("predict_train_",round(max(mymodel$nestimators)),".csv")
vval_pred  <- read.csv(vval_pred_file)
tval_pred  <- read.csv(tval_pred_file)

#Get original data
vval_original <- read.csv("valid_original.csv")
tval_original <- read.csv("train_original.csv")

#Errors
validation_errors <- vval_original$prediction - vval_pred$prediction
train_errors      <- tval_original$prediction - tval_pred$prediction
summary_valid <- summary(validation_errors)
summary_train <- summary(train_errors)
summary_valid_abs <- summary(abs(validation_errors))
summary_train_abs <- summary(abs(train_errors))
summary_df    <- as.data.frame(cbind(summary_train, summary_train_abs, summary_valid, summary_valid_abs))
colnames(summary_df) <- c("Train errors", "Absolute Train errors", "Validation errors", "Absolute Validation errors")

kable(summary_df)
```

## Example
You can run the model via the following command:

```{r results='markup', echo = FALSE, comment=NA}
dcommand <- paste0("docker run --rm -v ~/PATH/TO_FILE/YOU_WANT_TO_WORK_ON/:/data docker-solubility ", short_name)
cat(noquote(dcommand))
```


## See also
- [User's manual](https://github.com/RodrigoZepeda/docker-solubility/blob/master/Manual.md)
- [README](https://github.com/RodrigoZepeda/docker-solubility/blob/master/README.md)

